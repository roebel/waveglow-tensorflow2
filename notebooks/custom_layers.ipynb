{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invertible Convolution and WaveNet Custom Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "Start with standard imports as well as adding the scripts directory to the system path to allow custom imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "root_dir, _ = os.path.split(os.getcwd())\n",
    "script_dir = os.path.join(root_dir, 'scripts')\n",
    "sys.path.append(script_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hparams import hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invertible Convolution\n",
    "\n",
    "The training boolean in the call method can be used to run the layer in reverse.\n",
    "\n",
    "It could be worth investigating whether including the weight_norm wrapper of tensorflow addon can be used easily here and if it incurs significant improvements during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invertible1x1Conv(layers.Layer):\n",
    "  \"\"\"\n",
    "  Tensorflow 2.0 implementation of the inv1x1conv layer\n",
    "  Deprecated\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, filters, **kwargs):\n",
    "    super(Invertible1x1Conv, self).__init__(**kwargs)\n",
    "    self.kernel_size = filters\n",
    "    self.activation = tf.keras.activations.get(\"linear\")\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    \"\"\"This implementation assumes that the channel axis is last\"\"\"\n",
    "    self.kernel = self.add_weight(\n",
    "      shape=[1, self.filters, self.filters],\n",
    "      initializer=tf.initializers.orthogonal(),\n",
    "      trainable=True,\n",
    "      dtype=self.dtype,\n",
    "      name='kernel')\n",
    "    \n",
    "#   @tf.function  \n",
    "  def call(self, inputs, training=True):\n",
    "    \"\"\"Training flag should be working now\"\"\"\n",
    "    \n",
    "    if training:\n",
    "      \n",
    "      # sign, log_det_weights = tf.linalg.slogdet(\n",
    "      #   tf.cast(self.W, tf.float32))\n",
    "      \n",
    "      log_det_weights = tf.math.log(tf.math.abs(tf.linalg.det(\n",
    "        tf.cast(self.kernel, tf.float32))))\n",
    "      loss = - tf.cast(tf.reduce_sum(log_det_weights), \n",
    "                       dtype=self.dtype)\n",
    "      # sign, log_det_weights = tf.linalg.slogdet(self.W)\n",
    "  \n",
    "      # loss = - tf.reduce_sum(log_det_weights)\n",
    "      self.add_loss(loss)\n",
    "      tf.summary.scalar(name='loss',\n",
    "                       data=loss)\n",
    "         \n",
    "      output = tf.nn.conv1d(inputs, self.kernel, \n",
    "                            stride=1, padding='SAME')\n",
    "    \n",
    "    else:\n",
    "      if not hasattr(self, 'kernel_inverse'):\n",
    "        self.kernel_inverse = tf.cast(tf.linalg.inv(\n",
    "          tf.cast(self.kernel, tf.float64)), dtype=self.dtype)\n",
    "        \n",
    "      return tf.nn.conv1d(inputs, self.kernel_inverse, \n",
    "                            stride=1, padding='SAME')   \n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(Invertible1x1Conv, self).get_config()\n",
    "    config.update(kernel_size = self.kernel_size)\n",
    "    \n",
    "    return config\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inv1x1Conv(layers.Conv1D):\n",
    "  \"\"\"\n",
    "  Tensorflow 2.0 implementation of the inv1x1conv layer \n",
    "  directly subclassing the tensorflow Conv1D layer\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, filters, **kwargs):\n",
    "    super(Inv1x1Conv, self).__init__(\n",
    "      filters=filters,\n",
    "      kernel_size=1,\n",
    "      strides=1,\n",
    "      padding='SAME',\n",
    "      use_bias=False,\n",
    "      kernel_initializer=tf.initializers.orthogonal(),\n",
    "      activation=\"linear\",\n",
    "      **kwargs)\n",
    "  \n",
    "  def call(self, inputs, training=True):\n",
    "    if training:\n",
    "      sign, log_det_weights = tf.linalg.slogdet(\n",
    "        tf.cast(self.kernel, tf.float32))\n",
    "      loss = - tf.cast(tf.reduce_sum(log_det_weights), \n",
    "                       dtype=self.dtype)\n",
    "      self.add_loss(loss)\n",
    "      tf.summary.scalar(name='loss',\n",
    "                       data=loss)\n",
    "      return super(Inv1x1Conv, self).call(inputs)\n",
    "      \n",
    "    else:\n",
    "      if not hasattr(self, 'kernel_inverse'):\n",
    "        self.kernel_inverse = tf.cast(tf.linalg.inv(\n",
    "          tf.cast(self.kernel, tf.float64)), dtype=self.dtype)\n",
    "        \n",
    "      return tf.nn.conv1d(inputs, self.kernel_inverse, \n",
    "                            stride=1, padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nvidia WaveNet Implementation\n",
    "Difference with the original implementations :\n",
    "WaveNet convonlution need not be causal. \n",
    "No dilation size reset. \n",
    "Dilation doubles on each layer\n",
    "\n",
    "It could be worth investigating whether including the weight_norm wrapper of tensorflow addon incurs significant improvements during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetNvidia(layers.Layer):\n",
    "  \"\"\"\n",
    "  Wavenet Block as defined in the WaveGlow implementation from Nvidia\n",
    "  \n",
    "  WaveNet convonlution need not be causal. \n",
    "  No dilation size reset. \n",
    "  Dilation doubles on each layer.\n",
    "  \"\"\"\n",
    "  def __init__(self, n_in_channels, n_channels = 256, \n",
    "               n_layers = 12, kernel_size = 3, **kwargs):\n",
    "    super(WaveNetNvidia, self).__init__(**kwargs)\n",
    "    \n",
    "    assert(kernel_size % 2 == 1)\n",
    "    assert(n_channels % 2 == 0)\n",
    "    \n",
    "    self.n_layers = n_layers\n",
    "    self.n_channels = n_channels\n",
    "    self.n_in_channels = n_in_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    \n",
    "    self.in_layers = []\n",
    "    self.res_skip_layers = []\n",
    "    self.cond_layers = []\n",
    "    \n",
    "    self.start = tfa.layers.wrappers.WeightNormalization(\n",
    "      layers.Conv1D(filters=self.n_channels,\n",
    "                    kernel_size=1,\n",
    "                    dtype=self.dtype,\n",
    "                    name=\"start\"))\n",
    "\n",
    "    self.end = tfa.layers.wrappers.WeightNormalization(\n",
    "      layers.Conv1D(filters=2 * self.n_in_channels,\n",
    "                    kernel_size = 1,\n",
    "                    kernel_initializer=tf.initializers.zeros(),\n",
    "                    bias_initializer=tf.initializers.zeros(),\n",
    "                    dtype=self.dtype,\n",
    "                    name=\"end\"))\n",
    "\n",
    "    for index in range(self.n_layers):\n",
    "      dilation_rate = 2 ** index\n",
    "      \n",
    "      in_layer = tfa.layers.wrappers.WeightNormalization(\n",
    "        layers.Conv1D(filters=2 * self.n_channels,\n",
    "                      kernel_size= self.kernel_size,\n",
    "                      dilation_rate=dilation_rate,\n",
    "                      padding=\"SAME\",\n",
    "                      dtype=self.dtype,\n",
    "                      name=\"conv1D_{}\".format(index)))    \n",
    "      self.in_layers.append(in_layer)\n",
    "      \n",
    "      \n",
    "      cond_layer = tfa.layers.wrappers.WeightNormalization(\n",
    "        layers.Conv1D(filters = 2 * self.n_channels,\n",
    "                      kernel_size = 1,\n",
    "                      padding=\"SAME\",\n",
    "                      dtype=self.dtype,\n",
    "                      name=\"cond_{}\".format(index)))\n",
    "      self.cond_layers.append(cond_layer)\n",
    "      \n",
    "      if index < self.n_layers - 1:\n",
    "        res_skip_channels = 2 * self.n_channels\n",
    "      else:\n",
    "        res_skip_channels = self.n_channels\n",
    "        \n",
    "      res_skip_layer = tfa.layers.wrappers.WeightNormalization(\n",
    "        layers.Conv1D(\n",
    "          filters=res_skip_channels,\n",
    "          kernel_size=1,\n",
    "          dtype=self.dtype,\n",
    "          name=\"res_skip_{}\".format(index)))\n",
    "      \n",
    "      self.res_skip_layers.append(res_skip_layer)\n",
    "      \n",
    "    \n",
    "  def call(self, inputs):\n",
    "    \"\"\"\n",
    "    This implementatation does not require exposing a training boolean flag \n",
    "    as only the affine coupling behaviour needs reversing during\n",
    "    inference.\n",
    "    \"\"\"\n",
    "    audio_0, spect = inputs\n",
    "    \n",
    "    started = self.start (audio_0)   \n",
    "    \n",
    "    for index in range(self.n_layers):\n",
    "      in_layered = self.in_layers[index](started)\n",
    "      cond_layered = self.cond_layers[index](spect)\n",
    "      \n",
    "      half_tanh, half_sigmoid = tf.split(\n",
    "        in_layered + cond_layered, 2, axis=2)\n",
    "      half_tanh = tf.nn.tanh(half_tanh)\n",
    "      half_sigmoid = tf.nn.sigmoid(half_sigmoid)\n",
    "    \n",
    "      activated = half_tanh * half_sigmoid\n",
    "      \n",
    "      res_skip_activation = self.res_skip_layers[index](activated)\n",
    "      \n",
    "      if index < (self.n_layers - 1):\n",
    "        res_skip_activation_0, res_skip_activation_1 = tf.split(\n",
    "          res_skip_activation, 2, axis=2)\n",
    "        started = res_skip_activation_0 + started\n",
    "        skip_activation = res_skip_activation_1\n",
    "      else:\n",
    "        skip_activation = res_skip_activation\n",
    "\n",
    "      if index == 0:\n",
    "        output = skip_activation\n",
    "      else:\n",
    "        output = skip_activation + output\n",
    "        \n",
    "    output = self.end(output)\n",
    "    \n",
    "    log_s, bias = tf.split(output, 2, axis=2)\n",
    "    \n",
    "    return output\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(WaveNetBlock, self).get_config()\n",
    "    config.update(n_in_channels = self.n_in_channels)\n",
    "    config.update(n_channels = self.n_channels)\n",
    "    config.update(n_layers = self.n_layers)\n",
    "    config.update(kernel_size = self.kernel_size)\n",
    "  \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Affine Coupling Layer\n",
    "\n",
    "This layer does not have any trainable weights. It can be inverted by setting the training boolean to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCoupling(layers.Layer):\n",
    "  \"\"\"\n",
    "  Invertible Affine Layer\n",
    "  \n",
    "  The inverted behaviour is obtained by setting the training boolean\n",
    "  in the call method to false\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, **kwargs):\n",
    "    super(AffineCoupling, self).__init__(**kwargs)\n",
    "    \n",
    "  def call(self, inputs, training=None):\n",
    "    \n",
    "    audio_1, wavenet_output = inputs\n",
    "    \n",
    "    log_s, bias = tf.split(wavenet_output, 2, axis=2)\n",
    "    \n",
    "    if training:\n",
    "      audio_1 = audio_1 * tf.math.exp(log_s) + bias\n",
    "      loss = - tf.reduce_sum(log_s)\n",
    "      self.add_loss(loss)\n",
    "      tf.summary.scalar(name='loss', data=loss)\n",
    "    else:\n",
    "      audio_1 = (audio_1 - bias) *  tf.math.exp( - log_s)      \n",
    "    \n",
    "    return audio_1\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(AffineCoupling, self).get_config()\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveNet And Affine Coupling\n",
    "This block is a convenience block which has been defined to make it more straightforward to implement the WaveGlow model using the keras functional API. Note that affine coupling is the choice made in the original implementation of WaveGlow, but other choices are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetAffineBlock(layers.Layer):\n",
    "  \"\"\"\n",
    "  Wavenet + Affine Layer\n",
    "  Convenience block to provide a tidy model definition\n",
    "  \"\"\"\n",
    "  def __init__(self, n_in_channels, n_channels = 256,\n",
    "               n_layers = 12, kernel_size = 3, **kwargs):\n",
    "    super(WaveNetAffineBlock, self).__init__(**kwargs)\n",
    "    \n",
    "    self.n_layers =  n_layers\n",
    "    self.n_channels = n_channels\n",
    "    self.n_in_channels = n_in_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    \n",
    "    self.wavenet = WaveNetNvidia(n_in_channels=n_in_channels,\n",
    "                                 n_channels=n_channels,\n",
    "                                 n_layers=n_layers,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 dtype=self.dtype)\n",
    "    \n",
    "    self.affine_coupling = AffineCoupling(dtype=self.dtype)\n",
    "      \n",
    "    \n",
    "  def call(self, inputs, training=None):\n",
    "    \"\"\"\n",
    "    training should be set to false to inverse affine layer\n",
    "    \"\"\"\n",
    "    audio, spect = inputs\n",
    "    audio_0, audio_1 = tf.split(audio, 2, axis=2)\n",
    "    \n",
    "    wavenet_output = self.wavenet((audio_0, spect))\n",
    "    \n",
    "    audio_1 = self.affine_coupling(\n",
    "      (audio_1, wavenet_output), training=training)   \n",
    "         \n",
    "    audio = layers.Concatenate(axis=2) ([audio_0, audio_1])\n",
    "    \n",
    "    return audio\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super(WaveNetBlock, self).get_config()\n",
    "    config.update(n_in_channels = self.n_in_channels)\n",
    "    config.update(n_channels = self.n_channels)\n",
    "    config.update(n_layers = self.n_layers)\n",
    "    config.update(kernel_size = self.kernel_size)\n",
    "  \n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of WeightNormalisation\n",
    "This implementation has been copied from the tensorflow-addons library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightNormalization(tf.keras.layers.Wrapper):\n",
    "    \"\"\"This wrapper reparameterizes a layer by decoupling the weight's\n",
    "    magnitude and direction.\n",
    "    This speeds up convergence by improving the\n",
    "    conditioning of the optimization problem.\n",
    "    Weight Normalization: A Simple Reparameterization to Accelerate\n",
    "    Training of Deep Neural Networks: https://arxiv.org/abs/1602.07868\n",
    "    Tim Salimans, Diederik P. Kingma (2016)\n",
    "    WeightNormalization wrapper works for keras and tf layers.\n",
    "    ```python\n",
    "      net = WeightNormalization(\n",
    "          tf.keras.layers.Conv2D(2, 2, activation='relu'),\n",
    "          input_shape=(32, 32, 3),\n",
    "          data_init=True)(x)\n",
    "      net = WeightNormalization(\n",
    "          tf.keras.layers.Conv2D(16, 5, activation='relu'),\n",
    "          data_init=True)(net)\n",
    "      net = WeightNormalization(\n",
    "          tf.keras.layers.Dense(120, activation='relu'),\n",
    "          data_init=True)(net)\n",
    "      net = WeightNormalization(\n",
    "          tf.keras.layers.Dense(n_classes),\n",
    "          data_init=True)(net)\n",
    "    ```\n",
    "    Arguments:\n",
    "      layer: a layer instance.\n",
    "      data_init: If `True` use data dependent variable initialization\n",
    "    Raises:\n",
    "      ValueError: If not initialized with a `Layer` instance.\n",
    "      ValueError: If `Layer` does not contain a `kernel` of weights\n",
    "      NotImplementedError: If `data_init` is True and running graph execution\n",
    "      \n",
    "      Deprecated\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, data_init=True, **kwargs):\n",
    "        super(WeightNormalization, self).__init__(layer, **kwargs)\n",
    "        self.data_init = data_init\n",
    "        self._initialized = False\n",
    "        self._track_trackable(layer, name='layer')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build `Layer`\"\"\"\n",
    "        input_shape = tf.TensorShape(input_shape).as_list()\n",
    "        self.input_spec = tf.keras.layers.InputSpec(shape=input_shape)\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "\n",
    "            if not hasattr(self.layer, 'kernel'):\n",
    "                raise ValueError('`WeightNormalization` must wrap a layer that'\n",
    "                                 ' contains a `kernel` for weights')\n",
    "\n",
    "            # The kernel's filter or unit dimension is -1\n",
    "            self.layer_depth = int(self.layer.kernel.shape[-1])\n",
    "            self.kernel_norm_axes = list(\n",
    "                range(self.layer.kernel.shape.rank - 1))\n",
    "\n",
    "            self.v = self.layer.kernel\n",
    "            self.g = self.add_variable(\n",
    "                name=\"g\",\n",
    "                shape=(self.layer_depth,),\n",
    "                initializer=tf.keras.initializers.get('ones'),\n",
    "                dtype=self.layer.kernel.dtype,\n",
    "                trainable=True)\n",
    "\n",
    "        super(WeightNormalization, self).build()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Call `Layer`\"\"\"\n",
    "        if not self._initialized:\n",
    "            self._initialize_weights(inputs)\n",
    "\n",
    "        self._compute_weights()  # Recompute weights for each forward pass\n",
    "        output = self.layer(inputs)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(\n",
    "            self.layer.compute_output_shape(input_shape).as_list())\n",
    "\n",
    "    def _compute_weights(self):\n",
    "        \"\"\"Generate normalized weights.\n",
    "        This method will update the value of self.layer.kernel with the\n",
    "        normalized value, so that the layer is ready for call().\n",
    "        \"\"\"\n",
    "        with tf.name_scope('compute_weights'):\n",
    "            self.layer.kernel = tf.nn.l2_normalize(\n",
    "                self.v, axis=self.kernel_norm_axes) * self.g\n",
    "\n",
    "    def _initialize_weights(self, inputs):\n",
    "        \"\"\"Initialize weight g.\n",
    "        The initial value of g could either from the initial value in v,\n",
    "        or by the input value if self.data_init is True.\n",
    "        \"\"\"\n",
    "        if self.data_init:\n",
    "            self._data_dep_init(inputs)\n",
    "        else:\n",
    "            self._init_norm()\n",
    "        self._initialized = True\n",
    "\n",
    "    def _init_norm(self):\n",
    "        \"\"\"Set the weight g with the norm of the weight vector.\"\"\"\n",
    "        with tf.name_scope('init_norm'):\n",
    "            flat = tf.reshape(self.v, [-1, self.layer_depth])\n",
    "            self.g.assign(\n",
    "                tf.reshape(tf.linalg.norm(flat, axis=0), (self.layer_depth,)))\n",
    "\n",
    "    def _data_dep_init(self, inputs):\n",
    "        \"\"\"Data dependent initialization.\"\"\"\n",
    "\n",
    "        with tf.name_scope('data_dep_init'):\n",
    "            # Generate data dependent init values\n",
    "            existing_activation = self.layer.activation\n",
    "            self.layer.activation = None\n",
    "            x_init = self.layer(inputs)\n",
    "            data_norm_axes = list(range(x_init.shape.rank - 1))\n",
    "            m_init, v_init = tf.nn.moments(x_init, data_norm_axes)\n",
    "            scale_init = 1. / tf.math.sqrt(v_init + 1e-10)\n",
    "\n",
    "        # Assign data dependent init values\n",
    "        self.g = self.g * scale_init\n",
    "        if hasattr(self.layer, 'bias'):\n",
    "            self.layer.bias = -m_init * scale_init\n",
    "        self.layer.activation = existing_activation\n",
    "        \n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'data_init': self.data_init}\n",
    "        base_config = super(WeightNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Implementation of WeightNormalisedInvertible1x1Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inv1x1ConvWeightNorm(layers.Conv1D):\n",
    "  \n",
    "  \n",
    "  def __init__(self, filters, **kwargs):\n",
    "    super(Inv1x1ConvWeightNorm, self).__init__(\n",
    "      filters=filters,\n",
    "      kernel_size=1,\n",
    "      strides=1,\n",
    "      padding='SAME',\n",
    "      use_bias=False,\n",
    "      kernel_initializer=tf.initializers.orthogonal(),\n",
    "      activation=\"linear\",\n",
    "      **kwargs)\n",
    "    self._initialized = False\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    super(Inv1x1ConvWeightNorm, self).build(input_shape)\n",
    "    \n",
    "    self.layer_depth = self.filters\n",
    "    self.kernel_norm_axes = [0, 1]\n",
    "      \n",
    "    self.v = self.kernel\n",
    "    self.g = self.add_variable(\n",
    "        name=\"g\",\n",
    "        shape=self.layer_depth,\n",
    "        initializer=tf.keras.initializers.get('ones'),\n",
    "        dtype=self.dtype,\n",
    "        trainable=True)\n",
    "    \n",
    "    flat = tf.squeeze(self.v, axis=0)\n",
    "    self.g.assign(tf.linalg.norm(flat, axis=0))\n",
    "  \n",
    "  def call(self, inputs, training=True):\n",
    "    if training:\n",
    "      self.kernel = tf.nn.l2_normalize(\n",
    "        self.v, axis=self.kernel_norm_axes) * self.g\n",
    "      \n",
    "      sign, log_det_weights = tf.linalg.slogdet(\n",
    "        tf.cast(self.kernel, tf.float32))\n",
    "      loss = - tf.cast(tf.reduce_sum(log_det_weights), \n",
    "                       dtype=self.dtype)\n",
    "      self.add_loss(loss)\n",
    "      tf.summary.scalar(name='loss',\n",
    "                       data=loss)\n",
    "      return super(Inv1x1ConvWeightNorm, self).call(inputs)\n",
    "      \n",
    "    else:\n",
    "      if not hasattr(self, 'kernel_inverse'):\n",
    "        self.kernel_inverse = tf.cast(tf.linalg.inv(\n",
    "          tf.cast(self.kernel, tf.float64)), dtype=self.dtype)\n",
    "        \n",
    "      return tf.nn.conv1d(inputs, self.kernel_inverse, \n",
    "                            stride=1, padding='SAME')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
